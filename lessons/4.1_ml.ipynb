{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a02667",
   "metadata": {},
   "source": [
    "# **Machine Learning: Ordinary Least Squares**\n",
    "\n",
    "- __Part 1: Probability__ \n",
    "\n",
    "- __Part 2: Linear Algebra__\n",
    "\n",
    "- __Part 3: Statistics__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c11eb",
   "metadata": {},
   "source": [
    "## Part 1: Probability\n",
    "\n",
    "- **Conditional Distributions (Single Variable)**\n",
    "\n",
    "    - The distribution of a random variable $Y$ conditional on another random variable $X$ taking on a specific value is called the conditional distribution of $Y$ given $X$\n",
    "\n",
    "    - In general: $$Pr(Y = y | X = x) = \\frac{Pr(Y = y, X = x)}{Pr(X = x)}$$\n",
    "    \n",
    "    - In regression, we're interested in how $Y$ is distributed for each value of $X$. Ordinary Least Squares assumes that for each $X = x$, there's a conditional distribution of $Y$ with:\n",
    "\n",
    "        - A mean that depends on $x$\n",
    "\n",
    "        - Constant variance (homoskedasticity assumption)\n",
    "\n",
    "- **Conditional Expectations (Single Variable)**\n",
    "\n",
    "    - The conditional expectation is the expected value of $Y$, computed using the conditional distribution of $Y$ given $X$\n",
    "\n",
    "    - If $Y$ takes on $k$ values $y_1, y_2, \\ldots, y_k$, then the conditional mean of $Y$ given $X = x$ is:\n",
    "$$E[Y | X = x] = \\sum_{i=1}^{k} y_i \\cdot Pr(Y = y_i | X = x)$$\n",
    "\n",
    "- **Conditional Distributions (Multi-variable)**\n",
    "\n",
    "    - The distribution of the random variable $Y$ conditional on multiple random variables $X_1, X_2, \\ldots, X_p$ each taking on a specific value is called the conditional distrubtion of $Y$ given $X_1, X_2, \\ldots, X_p$\n",
    "    \n",
    "    - In general: $$Pr(Y = y | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = \\frac{Pr(Y = y, X_1 = x_1, \\ldots, X_p = x_p)}{Pr(X_1 = x_1, \\ldots, X_p = x_p)}$$\n",
    "\n",
    "    - In multiple regression, we model how $Y$ is distributed conditional on the entire set of predictor variables\n",
    "\n",
    "- **Conditional Expectations (Multi-variable)**\n",
    "\n",
    "    - The conditional expectation of $Y$ given multiple predictors is:\n",
    "    $$E[Y | X_1 = x_1, \\ldots, X_p = x_p] = \\sum_{i=1}^{k} y_i \\cdot Pr(Y = y_i | X_1 = x_1, \\ldots, X_p = x_p)$$\n",
    "\n",
    "    - In multiple linear regression, this becomes:\n",
    "    $$E[Y | X_1, \\ldots, X_p] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99ec78",
   "metadata": {},
   "source": [
    "## Part 2: Linear Algebra\n",
    "\n",
    "### Basic Linear Algebra Notation and Basic Computations\n",
    "\n",
    "- **Matrix**\n",
    "\n",
    "    - An _m × n_ rectangular array of entries \n",
    "\n",
    "- **Vector**\n",
    "\n",
    "    - Row Vector\n",
    "\n",
    "        - A _1 × n_ matrix consisting of n entries\n",
    "\n",
    "        - For instance let $\\mathbf{u}$ denote a _1 x n_ row vector,\n",
    "        $$\\mathbf{u} = [u_1, u_2, \\ldots, u_n]$$\n",
    "\n",
    "    - Column Vector\n",
    "\n",
    "        - An _m × 1_ matrix consisting of m entries\n",
    "\n",
    "        - For instance let $\\mathbf{v}$ denote a _m x 1_ column vector,\n",
    "        $$\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_m \\end{bmatrix}$$\n",
    "\n",
    "- **Inner Product**\n",
    "\n",
    "    - Commonly referred to as the dot product when $\\mathbf{u}$ and $\\mathbf{v}$ are in $\\mathbb{R}^n$\n",
    "\n",
    "    - The inner product results in a scalar\n",
    "\n",
    "    - The inner product of two vectors $\\mathbf{u}$ and $\\mathbf{v}$ is denoted by\n",
    "\n",
    "$$ \\langle \\mathbf{u}, \\mathbf{v} \\rangle = [u_1, u_2, \\ldots, u_n] \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\sum_{i=1}^{n} u_i v_i = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n $$\n",
    "\n",
    "- **Distance**\n",
    "\n",
    "    - For $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^n$, the distance between $\\mathbf{u}$ and $\\mathbf{v}$, is the length of the vector $\\mathbf{u} - \\mathbf{v}$\n",
    "\n",
    "    - Distance is denoted by\n",
    "\n",
    "$$\n",
    "\\text{dist}(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \\cdots + (u_n - v_n)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eca2c8",
   "metadata": {},
   "source": [
    "### Orthogonality\n",
    "\n",
    "- Two vectors are said to be orthogonal (perpendicular) if their inner product is zero \n",
    "\n",
    "- **Definition: Orthogonal Set**\n",
    "\n",
    "    - A set of vectors where each pair of distinct vectors from the set is orthogonal\n",
    "\n",
    "    - If $\\mathbf{u}_i \\cdot \\mathbf{v}_j = 0$, whenever $i \\neq j$\n",
    "\n",
    "- **Orthogonal Projection Formula**\n",
    "\n",
    "    - The orthogonal projection of $\\mathbf{y}$ onto $\\mathbf{L}$ is\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\text{proj}_\\mathbf{L}(\\mathbf{y}) = \\frac{\\mathbf{y} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}$$\n",
    "\n",
    "- **Orthogonal Projections**\n",
    "\n",
    "    - Given a vector $\\mathbf{y}$ and a subspace $\\mathbf{W}$ in $\\mathbb{R}^n$, there is a vector $\\hat{\\mathbf{y}}$ in $\\mathbf{W}$ such that\n",
    "\n",
    "        - $\\hat{\\mathbf{y}}$ is the unique vector in $\\mathbf{W}$ for which $\\mathbf{y} - \\hat{\\mathbf{y}}$ is orthogonal to $\\mathbf{W}$, and\n",
    "\n",
    "        - $\\hat{\\mathbf{y}}$ is the unique vector in $\\mathbf{W}$ closest to $\\mathbf{y}$\n",
    "\n",
    "- **Orthogonal Decomposition Formula**\n",
    "\n",
    "    - Let $\\mathbf{W}$ be a subspace of $\\mathbb{R}^n$. Then each $\\mathbf{y}$ in $\\mathbb{R}^n$ can be written uniquely in the form,\n",
    "    $\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{z}$. Where $\\hat{\\mathbf{y}}$ is in $\\mathbf{W}$ and $\\mathbf{z}$ is in $\\mathbf{W}^\\perp$\n",
    "\n",
    "    - In fact, if $\\set{\\mathbf{u_1}, \\mathbf{u_2}, \\ldots, \\mathbf{u_n}}$ is any orthogonal basis of $\\mathbf{W}$ then,\n",
    "\n",
    "    $$\\hat{\\mathbf{y}} = \\frac{\\mathbf{y} \\cdot \\mathbf{u_1}}{\\mathbf{u_1} \\cdot \\mathbf{u_1}}\\mathbf{u_1} + \\frac{\\mathbf{y} \\cdot \\mathbf{u_2}}{\\mathbf{u_2} \\cdot \\mathbf{u_2}}\\mathbf{u_2} + \\ldots + \\frac{\\mathbf{y} \\cdot \\mathbf{u_n}}{\\mathbf{u_n} \\cdot \\mathbf{u_n}}\\mathbf{u_n} $$\n",
    "\n",
    "    - And, \n",
    "    $$\\mathbf{z} = \\mathbf{y} - \\hat{\\mathbf{y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a42aad",
   "metadata": {},
   "source": [
    "### Orthogonal Projection of $\\mathbf{y}$ onto $\\mathbf{W}$\n",
    "<img src=\"/Users/henrycosentino/Desktop/Python/quant_mentorship/images/orthogonal_projection.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094abeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0ceefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "\n",
      "y_hat:\n",
      " [0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Example of an Orthogonal Projection Decomposition\n",
    "\n",
    "# Basis for W (matrix)\n",
    "W = [np.array([1, 0, 0]), np.array([0, 1, 0]), np.array([0, 0, 1])]\n",
    "\n",
    "# y (vector)\n",
    "y = np.array([0, 1, 1])\n",
    "\n",
    "# y_hat (vector)\n",
    "y_hat = y_hat = sum((np.dot(y, u) / np.dot(u, u)) * u for u in W) # <--- Decomposition formula applied to find y_hat\n",
    "\n",
    "print(\"W:\\n\", np.matrix(W))\n",
    "print(\"\\n\")\n",
    "print(\"y_hat:\\n\", y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb11f35",
   "metadata": {},
   "source": [
    "### Orthogonality (cont'd)\n",
    "\n",
    "- **The Best Approximation Theorem**\n",
    "    \n",
    "    - Let $\\mathbf{W}$ be a subspace of $\\mathbf{R}^n$, let $\\mathbf{y}$ be any vector in $\\mathbf{R}^n$, and let $\\hat{\\mathbf{y}}$ be the orthogonal projection of $\\mathbf{y}$ onto $\\mathbf{W}$. Then $\\hat{\\mathbf{y}}$ is the closest point in $\\mathbf{W}$ to $\\mathbf{y}$, in the sense that\n",
    "\n",
    "    $$ \\|\\mathbf{y} - \\hat{\\mathbf{y}}\\| < \\|\\mathbf{y} - \\mathbf{v}\\|, \\forall\\mathbf{v}\\in\\mathbf{W} \\text{ where } \\mathbf{v} \\ne \\hat{\\mathbf{y}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33323910",
   "metadata": {},
   "source": [
    "### Best Approximation Theorem\n",
    "<img src=\"/Users/henrycosentino/Desktop/Python/quant_mentorship/images/best_approximation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15fe8f",
   "metadata": {},
   "source": [
    "### Linear Algebra Notation (cont'd)\n",
    "\n",
    "- **Matrix-Vector Equation**\n",
    "\n",
    "    - Let $\\mathbf{A}$ be an _m × n_ real valued matrix, $\\mathbf{x}$ be an _n × 1_ real valued column vector, and $\\mathbf{b}$ be an _m × 1_ real valued column vector\n",
    "    \n",
    "    - Then, a matrix-vector equation is denoted by $$\\mathbf{Ax} = \\mathbf{b}$$\n",
    "\n",
    "    - If there exists an $\\mathbf{x}$ such that $\\mathbf{Ax} = \\mathbf{b}$ holds, then $\\mathbf{Ax} = \\mathbf{b}$ is said to be _consistent_\n",
    "\n",
    "    - If there is no such $\\mathbf{x}$ such that $\\mathbf{Ax} = \\mathbf{b}$ holds, then $\\mathbf{Ax} = \\mathbf{b}$ is said to be _inconsistent_\n",
    "\n",
    "- **Matrix-Vector Equation as a System of Linear Equations**\n",
    "\n",
    "    - $\\mathbf{Ax} = \\mathbf{b}$ can be decomposed into\n",
    "    \n",
    "$$\\begin{align}\n",
    "a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n",
    "&\\vdots \\\\\n",
    "a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &= b_m\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a0b59",
   "metadata": {},
   "source": [
    "### Least Squares Notation & Solution\n",
    "\n",
    "- **Notation**\n",
    "\n",
    "    - Instead of the typical notation $\\mathbf{Ax} = \\mathbf{b}$, we can rewrite this in a more familiar manner as:\n",
    "\n",
    "    $$\\mathbf{X\\beta} = \\mathbf{y}$$\n",
    "\n",
    "    - Where:\n",
    "\n",
    "        - $\\mathbf{X}$ represents an _n × (m+1)_ matrix of our data (independent variables, known values)\n",
    "\n",
    "        - $\\boldsymbol{\\beta}$ represents an _(m+1) × 1_ matrix of weights (coefficients, unknown values)\n",
    "\n",
    "        - $\\mathbf{y}$ represents an _n × 1_ matrix of our observation data (dependent variable, known values)\n",
    "\n",
    "    - $\\mathbf{X\\beta} = \\mathbf{y}$ can be further decomposed:\n",
    "\n",
    "    $$\\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1m} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2m} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{nm} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_m \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n",
    "\n",
    "- **Solution**\n",
    "\n",
    "    - Inconsistent systems arise often in applications. Thus, we are not always guaranteed a vector $\\boldsymbol{\\beta}$ that satisfies $\\mathbf{X\\beta} = \\mathbf{y}$\n",
    "\n",
    "    - We notice that to approximate a solution to $\\mathbf{X\\beta} = \\mathbf{y}$ we look for a vector in the span of $\\mathbf{X}$ that is \"closest\" to $\\mathbf{y}$. Then the closest solution for $\\boldsymbol{\\beta}$ will be the coefficients of the columns of $\\mathbf{X}$ that give this vector in the column space of $\\mathbf{X}$\n",
    "\n",
    "    - In terms of the definition of projection we are looking for a solution to:\n",
    "    \n",
    "    $$\\tag{1} \\mathbf{X\\beta} = \\text{proj}_{\\text{col}(\\mathbf{X})}(\\mathbf{y})$$\n",
    "\n",
    "    - We call the solution $\\hat{\\boldsymbol{\\beta}}$, we let $\\hat{\\mathbf{y}} = \\text{proj}_{\\text{col}(\\mathbf{X})}(\\mathbf{y})$, and so the solution we are looking for $(1)$ becomes $$\\mathbf{X\\hat{\\beta}} = \\mathbf{\\hat{y}}$$\n",
    "\n",
    "    - By the orthogonal decomposition theorem we know that $\\mathbf{y} - \\hat{\\mathbf{y}}$ is orthogonal to the column space of $\\mathbf{X}$, or $\\mathbf{x}_j \\cdot (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0$\n",
    "\n",
    "    - By the definition of the inner product this means that $\\mathbf{x}_j^T \\cdot (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0$ for each column of $\\mathbf{X}$. But these are the rows of $\\mathbf{X}^T$, and so $\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0$\n",
    "\n",
    "    - Thus, the _normal equations_ are given by\n",
    "\n",
    "    $$\\mathbf{X}^T\\mathbf{y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\mathbf{\\beta}}$$\n",
    "\n",
    "    - Notice, the _normal equations_ only have one unknown, $\\hat{\\mathbf{\\beta}}$, which can be easily solved for by row reduction (Gaussian elimination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cafa60",
   "metadata": {},
   "source": [
    "### Least Squares Projection\n",
    "<img src=\"/Users/henrycosentino/Desktop/Python/quant_mentorship/images/least_squares_proj.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2a1d9",
   "metadata": {},
   "source": [
    "### Least Squares (cont'd)\n",
    "\n",
    "- **Least Squares Error of Approximation**\n",
    "\n",
    "    - The least squares error of approximation can be denoted by:\n",
    "    \n",
    "    $$\\|\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\|$$\n",
    "\n",
    "    - Notice that $\\mathbf{y}$ is a vector of the observed data. In a sense we are comparing the distance of our solution to the observed data\n",
    "\n",
    "- **General Linear Models**\n",
    "\n",
    "    - The matrix equation is still $\\mathbf{X\\beta} = \\mathbf{y}$, but the specific form of $\\mathbf{X}$ changes from problem to problem due to the design of the model and the available data\n",
    "\n",
    "    - Thus, statisticians usually introduce a residual vector $\\boldsymbol{\\epsilon}$, defined by:\n",
    "    \n",
    "    $$\\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{X\\beta}$$\n",
    "\n",
    "    - So, the model becomes:\n",
    "    \n",
    "    $$\\mathbf{y} = \\mathbf{X\\beta} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "    - The goal is to minimize the length of $\\boldsymbol{\\epsilon}$\n",
    "\n",
    "- **Least Squares of Other Curves**\n",
    "\n",
    "    - When the data points $(x_1, y_1), \\ldots, (x_n, y_n)$ do not lie close to any 'straight' line, it may be appropriate to postulate some other functional relationship between $x$ (the independent variable) and $y$ (the dependent variable)\n",
    "\n",
    "    - One such form:\n",
    "    \n",
    "    $$y = \\beta_0 f_0(x) + \\beta_1 f_1(x) + \\cdots + \\beta_k f_k(x) \\tag{2}$$\n",
    "\n",
    "    - Where:\n",
    "\n",
    "        - $\\beta_0, \\beta_1, \\ldots, \\beta_k$ are parameters (coefficients) that must be determined\n",
    "\n",
    "        - $f_0(x), f_1(x), \\ldots, f_k(x)$ are known functions\n",
    "\n",
    "    - Notice that $(2)$ is a linear model because it is linear in terms of the parameters $\\beta_0, \\beta_1, \\ldots, \\beta_k$\n",
    "\n",
    "    - The solution to least squares of other curves is similar, and involves the _normal equations_\n",
    "\n",
    "    - Example:\n",
    "\n",
    "        - Consider the polynomial linear regression model of degree three:\n",
    "        \n",
    "        $$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$$\n",
    "\n",
    "        - Which can be rewritten as:\n",
    "        \n",
    "        $$\\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 \\\\ 1 & x_2 & x_2^2 & x_2^3 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 & x_n^3 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n",
    "\n",
    "- **Multiple Linear Regression**\n",
    "\n",
    "    - Experiments involving more than one independent variable\n",
    "\n",
    "    - Generally:\n",
    "\n",
    "    $$y = \\beta_0 f_0(u,v,\\ldots) + \\beta_1 f_1(u,v,\\ldots) + \\cdots + \\beta_k f_k(u,v,\\ldots)$$\n",
    "\n",
    "    - Where:\n",
    "\n",
    "        - $\\beta_0, \\beta_1, \\ldots, \\beta_k$ are parameters (coefficients) that must be determined\n",
    "\n",
    "        - $f_0(u,v,\\ldots), f_1(u,v,\\ldots), \\ldots, f_k(u,v,\\ldots)$ are known functions\n",
    "\n",
    "    - The solution for multiple linear regression is similar, and involves the _normal equations_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fff3cf",
   "metadata": {},
   "source": [
    "### Example One\n",
    "\n",
    "Find a least squares solution and least squares error of the below system by hand.\n",
    "\n",
    "$$x_1 + x_2 = 1$$\n",
    "$$x_1 + 2x_2 = 3$$\n",
    "$$x_1 + 3x_2 = 2$$\n",
    "\n",
    "### Example Two\n",
    "\n",
    "Find a least squares solution for the below points in quadratic form through the origin.\n",
    "\n",
    "$$(1, 1), (2, 5), (-1, 2)$$\n",
    "\n",
    "### Example Three\n",
    "\n",
    "Find a least squares solution and least squares error of the below data using NumPy.\n",
    "\n",
    "- [Data Description](https://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/EE_Datasets/Growth_Description.pdf)\n",
    "\n",
    "    - _growth_: Average annual percentage growth of real Gross Domestic Product (GDP)* from\n",
    "1960 to 1995\n",
    "\n",
    "    - _yearsschool_: Average number of years of schooling of adult residents in that country in 1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04183bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>growth</th>\n",
       "      <th>oil</th>\n",
       "      <th>rgdp60</th>\n",
       "      <th>tradeshare</th>\n",
       "      <th>yearsschool</th>\n",
       "      <th>rev_coups</th>\n",
       "      <th>assasinations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India</td>\n",
       "      <td>1.915168</td>\n",
       "      <td>0</td>\n",
       "      <td>765.999817</td>\n",
       "      <td>0.140502</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>0.617645</td>\n",
       "      <td>0</td>\n",
       "      <td>4462.001465</td>\n",
       "      <td>0.156623</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japan</td>\n",
       "      <td>4.304759</td>\n",
       "      <td>0</td>\n",
       "      <td>2953.999512</td>\n",
       "      <td>0.157703</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>2.930097</td>\n",
       "      <td>0</td>\n",
       "      <td>1783.999878</td>\n",
       "      <td>0.160405</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>1.712265</td>\n",
       "      <td>0</td>\n",
       "      <td>9895.003906</td>\n",
       "      <td>0.160815</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>5.384184</td>\n",
       "      <td>0</td>\n",
       "      <td>2037.000366</td>\n",
       "      <td>0.979355</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>4.114544</td>\n",
       "      <td>0</td>\n",
       "      <td>1420.000244</td>\n",
       "      <td>1.105364</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>2.651335</td>\n",
       "      <td>0</td>\n",
       "      <td>5495.001953</td>\n",
       "      <td>1.115917</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mauritius</td>\n",
       "      <td>3.024178</td>\n",
       "      <td>0</td>\n",
       "      <td>2861.999268</td>\n",
       "      <td>1.127937</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Malta</td>\n",
       "      <td>6.652838</td>\n",
       "      <td>0</td>\n",
       "      <td>1374.000000</td>\n",
       "      <td>1.992616</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country_name    growth  oil       rgdp60  tradeshare  yearsschool  \\\n",
       "0           India  1.915168    0   765.999817    0.140502         1.45   \n",
       "1       Argentina  0.617645    0  4462.001465    0.156623         4.99   \n",
       "2           Japan  4.304759    0  2953.999512    0.157703         6.71   \n",
       "3          Brazil  2.930097    0  1783.999878    0.160405         2.89   \n",
       "4   United States  1.712265    0  9895.003906    0.160815         8.66   \n",
       "..            ...       ...  ...          ...         ...          ...   \n",
       "60         Cyprus  5.384184    0  2037.000366    0.979355         4.29   \n",
       "61       Malaysia  4.114544    0  1420.000244    1.105364         2.34   \n",
       "62        Belgium  2.651335    0  5495.001953    1.115917         7.46   \n",
       "63      Mauritius  3.024178    0  2861.999268    1.127937         2.44   \n",
       "64          Malta  6.652838    0  1374.000000    1.992616         5.64   \n",
       "\n",
       "    rev_coups  assasinations  \n",
       "0    0.133333       0.866667  \n",
       "1    0.933333       1.933333  \n",
       "2    0.000000       0.200000  \n",
       "3    0.100000       0.100000  \n",
       "4    0.000000       0.433333  \n",
       "..        ...            ...  \n",
       "60   0.100000       0.166667  \n",
       "61   0.033333       0.033333  \n",
       "62   0.000000       0.000000  \n",
       "63   0.000000       0.000000  \n",
       "64   0.000000       0.000000  \n",
       "\n",
       "[65 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in data\n",
    "\n",
    "fh = \"/Users/henrycosentino/Desktop/Python/quant_mentorship/data/growth_data.csv\"\n",
    "\n",
    "df = pd.read_csv(fh)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558c7e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65 entries, 0 to 64\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   country_name   65 non-null     object \n",
      " 1   growth         65 non-null     float64\n",
      " 2   oil            65 non-null     int64  \n",
      " 3   rgdp60         65 non-null     float64\n",
      " 4   tradeshare     65 non-null     float64\n",
      " 5   yearsschool    65 non-null     float64\n",
      " 6   rev_coups      65 non-null     float64\n",
      " 7   assasinations  65 non-null     float64\n",
      "dtypes: float64(6), int64(1), object(1)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "572ab4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.45000005],\n",
       "       [1.        , 4.98999977],\n",
       "       [1.        , 6.71000004],\n",
       "       [1.        , 2.8900001 ],\n",
       "       [1.        , 8.65999985]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign independent and dependent variables\n",
    "\n",
    "X = df[['yearsschool']].values # <-- The \".values\" attribute gives us the values and not a data frame\n",
    "X = np.column_stack([np.ones(len(X)), X]) # <-- Here we are adding a column of ones for the intercept\n",
    "\n",
    "y = df[['growth']].values\n",
    "\n",
    "X[:5 , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.1096\n",
      "Error: 14.3215\n",
      "Intercept: 0.9583\n",
      "Slope (yearsschool): 0.2470\n"
     ]
    }
   ],
   "source": [
    "# Solving using NumPy\n",
    "\n",
    "coefficients = np.linalg.solve(X.T @ X, X.T @ y) # <-- We use \"@\" for matrix multiplication and \".T\" to take the transpose\n",
    "\n",
    "e = y - X @ coefficients\n",
    "error = np.linalg.norm(e)\n",
    "\n",
    "rss = error**2\n",
    "tss = np.sum((y - np.mean(y))**2)\n",
    "r_squared = 1 - (rss / tss)\n",
    "\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n",
    "print(f\"Error: {error:.4f}\")\n",
    "print(f\"Intercept: {coefficients[0][0]:.4f}\")\n",
    "print(f\"Slope (yearsschool): {coefficients[1][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a57f46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.1096\n",
      "Intercept: 0.9583\n",
      "Slope (yearsschool): 0.2470\n"
     ]
    }
   ],
   "source": [
    "# Verifying answers with sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(f\"R-squared: {model.score(X, y):.4f}\")\n",
    "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
    "print(f\"Slope (yearsschool): {model.coef_[0][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771e228",
   "metadata": {},
   "source": [
    "## Part 3: Statistics\n",
    "\n",
    "- **The Gauss-Markov Theorem**\n",
    "\n",
    "    - Theorem: \n",
    "\n",
    "        - The Ordinary Least Squares (OLS) estimate for $\\hat{\\mathbf{\\beta}}$ in the linear model $\\mathbf{y} = \\mathbf{X\\beta} + \\mathbf{\\epsilon}$ is given by $\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$. If we assume that the errors have a mean $\\mathbf{0}$ and covariance $\\sigma^2\\mathbf{I}$, then the OLS estimator achieves the lowest sampling variance among all linear unbiased estimators for $\\mathbf{\\beta}$; that is, the OLS estimator is the _best linear unbiased estimator_ (BLUE) under the above statement\n",
    "\n",
    "    - Assumptions:\n",
    "\n",
    "        1. Linearity in Parameters: The dependent variable is a linear function of the parameters\n",
    "\n",
    "        2. Random Sampling: The data is obtained through random sampling from the population, so observations are _independently identically distributed_\n",
    "\n",
    "        3. No Perfect Collinearity: No independent variable is a perfect linear combination of the others\n",
    "\n",
    "        4. Zero Conditional Mean (exogeneity):\n",
    "\n",
    "            - $E[\\mathbf{\\epsilon} | \\mathbf{X}] = \\mathbf{0}$\n",
    "\n",
    "            - The error term has an expected value of zero given any value of the independent variables\n",
    "\n",
    "        5. Homoskedasticity\n",
    "\n",
    "            - $\\text{var}[\\mathbf{\\epsilon} | \\mathbf{X}] = \\sigma^2\\mathbf{I}$\n",
    "\n",
    "            - The variance of the error term is constant across all observations\n",
    "\n",
    "            - Errors have the same variance regardless of the $\\mathbf{X}$ values\n",
    "\n",
    "        6. No Autocorrelation (time series specific)\n",
    "\n",
    "            - $\\text{cov}[\\epsilon_i, \\epsilon_j | \\mathbf{X}] = 0$, when $i \\neq j$\n",
    "\n",
    "            - Error terms are uncorrelated with each other\n",
    "\n",
    "    - In a sense, when using OLS, the Gauss-Markov Theorem tells us that we are extracting the maximum amount of information from the data in the most efficient manner. In other words, OLS is optimal, but in reality it is optimal in a world that does not exist\n",
    "\n",
    "    - Proof of Unbiasedness (assumption 4)\n",
    "\n",
    "We aim to show that $E[\\hat{\\mathbf{\\beta}}] = \\mathbf{\\beta}$.\n",
    "\n",
    "Consider the OLS estimator:\n",
    "$$\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "Substitute the population model $\\mathbf{y} = \\mathbf{X\\beta} + \\mathbf{\\epsilon}$ into the estimator:\n",
    "$$\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X\\beta} + \\mathbf{\\epsilon})$$\n",
    "\n",
    "Distribute the terms:\n",
    "$$\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{X})\\mathbf{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{\\epsilon}$$\n",
    "\n",
    "Since $(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{X}) = \\mathbf{I}$, we simplify to:\n",
    "$$\\hat{\\mathbf{\\beta}} = \\mathbf{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{\\epsilon}$$\n",
    "\n",
    "Apply the conditional expectation $E[\\cdot | \\mathbf{X}]$. Because $\\mathbf{\\beta}$ is a fixed parameter and we are conditioning on $\\mathbf{X}$, both are treated as non-stochastic in this step:\n",
    "$$E[\\hat{\\mathbf{\\beta}} | \\mathbf{X}] = \\mathbf{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T E[\\mathbf{\\epsilon} | \\mathbf{X}]$$\n",
    "\n",
    "Apply the zero conditional mean assumption ($E[\\mathbf{\\epsilon} | \\mathbf{X}] = \\mathbf{0}$):\n",
    "$$E[\\hat{\\mathbf{\\beta}} | \\mathbf{X}] = \\mathbf{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{0})$$\n",
    "$$E[\\hat{\\mathbf{\\beta}} | \\mathbf{X}] = \\mathbf{\\beta}$$\n",
    "\n",
    "Finally, apply the Law of Iterated Expectations to find the unconditional expectation:\n",
    "$$E[\\hat{\\mathbf{\\beta}}] = E[E[\\hat{\\mathbf{\\beta}} | \\mathbf{X}]] = E[\\mathbf{\\beta}]$$\n",
    "\n",
    "Since $\\mathbf{\\beta}$ is a constant vector of population parameters, $E[\\mathbf{\\beta}] = \\mathbf{\\beta}$, therefore:\n",
    "$$E[\\hat{\\mathbf{\\beta}}] = \\mathbf{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877c6a9",
   "metadata": {},
   "source": [
    "- **Maximum Likelihood Estimator**\n",
    "\n",
    "    - Likelihood Function\n",
    "\n",
    "        - The information in the sample and the parameter $\\theta$ are involved in the joint distribution of the random sample, $$\\prod_{i=1}^{n} f(x_i ; \\theta)$$\n",
    "\n",
    "        - Here $f$ is any probability density function\n",
    "\n",
    "        - We want to view this as a function of $\\theta$, so we can write it as, $$L(\\theta) = L(\\theta; x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^{n} f(x_i ; \\theta)$$\n",
    "\n",
    "    - Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "        - An often-used estimate is the value of $\\theta$ that provides a maximum of our function $L(\\theta)$\n",
    "\n",
    "        - If $\\hat{\\theta}$ is unique, then we denote it by $\\hat{\\theta}$, and so, $$\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta)$$\n",
    "\n",
    "        - In a sense we are looking for a peak of the $L(\\theta)$ function\n",
    "\n",
    "    - Log-Likelihood\n",
    "\n",
    "        - In practice, it is often much easier to work with the log likelihood, $$\\ell(\\theta) = \\ln[L(\\theta)]$$\n",
    "\n",
    "        - Since $\\ln$ is a strictly increasing function, the value that maximizes $\\ell(\\theta)$ is the same value that maximizes $L(\\theta)$\n",
    "\n",
    "        - If we assume the probability density function is a differentiable function of $\\theta$ (which it usually is), then $\\hat{\\theta}$ frequently solves, $$\\frac{\\partial \\ell(\\theta)}{\\partial \\theta} = 0$$\n",
    "\n",
    "    - OLS & MLE\n",
    "\n",
    "        - MLE is not OLS. However, in regression modeling, the maximum likelihood function finds the optimal values of the intercept and slope of the line that maximizes the likelihood. In other words, MLE can be applied to solve for the parameters in OLS\n",
    "\n",
    "        - In terms of OLS, $\\theta$ will be a vector of parameters, $$\\theta = [\\beta_0, \\beta_1, \\sigma]$$\n",
    "\n",
    "        - And, the probability density function will follow that of a normal distribution, $$f(x_i ; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2}(\\frac{(x_i - \\mu)}{\\sigma})^2}$$\n",
    "\n",
    "        - In other words, for the MLE estimates of regression coefficients ($\\beta$) to be identical to OLS estimates, we must assume that the error terms ($\\epsilon$) follow a Normal distribution with a mean of zero and constant variance (homoscedasticity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b62c7",
   "metadata": {},
   "source": [
    "- **Statistical Modeling to Machine Learning**\n",
    "\n",
    "    - Two Different Goals\n",
    "\n",
    "        - Causation: Statisticians and econometricians usually seek to answer questions, and in doing so answer problems of causality\n",
    "\n",
    "        - Prediction: Data scientists typically seek to predict a specified value(s), often with limited explanatory power\n",
    "\n",
    "    - Statistical Inference\n",
    "\n",
    "        - Goal: causal models with explanatory power\n",
    "\n",
    "        - Typical Process\n",
    "\n",
    "            1. Statement of hypothesis\n",
    "\n",
    "            2. Specification of the mathematical model of theory\n",
    "\n",
    "            3. Specification of the statistical model of theory\n",
    "\n",
    "            4. Obtaining the data\n",
    "\n",
    "            5. Estimation of the statistical model's parameters\n",
    "\n",
    "            6. Determining whether to accept or reject the initial hypothesis\n",
    "\n",
    "        - Typically embody a probabilistic framework, are expressed linearly, not scalable (limited to lower dimensional data), prone to overfitting, and have extensive model diagnostics\n",
    "\n",
    "    - Machine Learning\n",
    "\n",
    "        - Goal: prediction performance, usually with low explanatory power\n",
    "\n",
    "        - Typical Process\n",
    "\n",
    "            1. Define the prediction problem and target variable\n",
    "\n",
    "            2. Collect and prepare the data (cleaning, feature engineering)\n",
    "\n",
    "            3. Split data into training, validation, and test sets\n",
    "\n",
    "            4. Select and train candidate models/algorithms\n",
    "\n",
    "            5. Tune hyperparameters and evaluate performance on validation set\n",
    "\n",
    "            6. Select best model and assess final performance on test set\n",
    "\n",
    "        - Typically embody probabilistic and algorithmic frameworks, can handle non-linear relationships, scalable to high-dimensional data, less prone to overfitting (through regularization and cross-validation), and focus on predictive accuracy over interpretability\n",
    "\n",
    "    - Therefore, ordinary least squares can be considered a type of parametric, supervised, linear regression machine learning model if used in the aforementioned predictive capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3fa71",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. **Hogg, R. V., McKean, J. W., & Craig, A. T.** (2019). *Introduction to Mathematical Statistics* (8th ed.). Pearson.\n",
    "\n",
    "2. **Lay, D. C., Lay, S. R., & McDonald, J. J.** (2021). *Linear Algebra and Its Applications* (6th ed., Global Edition).\n",
    "\n",
    "3. **Stock, J. H., & Watson, M. W.** (2020). *Introduction to Econometrics* (4th ed., Global Edition).\n",
    "\n",
    "4. **Jiang, Y.** (2023). *STA 211: The Mathematics of Regression - Lecture 8: The Gauss-Markov Theorem*. Department of Statistical Science, Duke University.\n",
    "\n",
    "5. **Jiang, Y.** (2023). *STA 211: The Mathematics of Regression - Lecture 9: Maximum Likelihood Estimation*. Department of Statistical Science, Duke University."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
